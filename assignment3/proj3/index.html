<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Assignment 3</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 id="assignment-3">Assignment 3</h1>
<p>Liwei Yang, <a href="mailto:liweiy@andrew.cmu.edu">liweiy@andrew.cmu.edu</a></p>
<p>Collaborators: fuchengp, jiamuz, jinkaiq</p>
<ul>
<li><a href="#assignment-3">Assignment 3</a></li>
<li><a href="#a-neural-volume-rendering-80-points">A. Neural Volume Rendering (80 points)</a>
<ul>
<li><a href="#0-transmittance-calculation-10-points">0. Transmittance Calculation (10 points)</a></li>
<li><a href="#1-differentiable-volume-rendering">1. Differentiable Volume Rendering</a>
<ul>
<li><a href="#11-familiarize-yourself-with-the-code-structure">1.1. Familiarize yourself with the code structure</a></li>
<li><a href="#12-outline-of-tasks">1.2. Outline of tasks</a></li>
<li><a href="#13-ray-sampling-5-points">1.3. Ray sampling (5 points)</a></li>
<li><a href="#14-point-sampling-5-points">1.4. Point sampling (5 points)</a></li>
<li><a href="#15-volume-rendering-20-points">1.5. Volume rendering (20 points)</a></li>
</ul>
</li>
<li><a href="#2-optimizing-a-basic-implicit-volume">2. Optimizing a basic implicit volume</a>
<ul>
<li><a href="#21-random-ray-sampling-5-points">2.1. Random ray sampling (5 points)</a></li>
<li><a href="#22-loss-and-training-5-points">2.2. Loss and training (5 points)</a></li>
<li><a href="#23-visualization">2.3. Visualization</a></li>
</ul>
</li>
<li><a href="#3-optimizing-a-neural-radiance-field-nerf-20-points">3. Optimizing a Neural Radiance Field (NeRF) (20 points)</a></li>
<li><a href="#4-nerf-extras-choose-one-more-than-one-is-extra-credit">4. NeRF Extras (CHOOSE ONE! More than one is extra credit)</a>
<ul>
<li><a href="#41-view-dependence-10-points">4.1 View Dependence (10 points)</a></li>
<li><a href="#42-coarsefine-sampling-10-points">4.2 Coarse/Fine Sampling (10 points)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#b-neural-surface-rendering-50-points">B. Neural Surface Rendering (50 points)</a>
<ul>
<li><a href="#5-sphere-tracing-10-points">5. Sphere Tracing (10 points)</a></li>
<li><a href="#6-optimizing-a-neural-sdf-15-points">6. Optimizing a Neural SDF (15 points)</a></li>
<li><a href="#7-volsdf-15-points">7. VolSDF (15 points)</a></li>
<li><a href="#8-neural-surface-extras-choose-one-more-than-one-is-extra-credit">8. Neural Surface Extras (CHOOSE ONE! More than one is extra credit)</a>
<ul>
<li><a href="#81-render-a-large-scene-with-sphere-tracing-10-points">8.1. Render a Large Scene with Sphere Tracing (10 points)</a></li>
<li><a href="#82-fewer-training-views-10-points">8.2 Fewer Training Views (10 points)</a></li>
<li><a href="#83-alternate-sdf-to-density-conversions-10-points">8.3 Alternate SDF to Density Conversions (10 points)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="a-neural-volume-rendering-80-points">A. Neural Volume Rendering (80 points)</h1>
<h2 id="0-transmittance-calculation-10-points">0. Transmittance Calculation (10 points)</h2>
<p><img src="data/t_cal.png" alt="t_cal"></p>
<h2 id="1-differentiable-volume-rendering">1. Differentiable Volume Rendering</h2>
<h3 id="11-familiarize-yourself-with-the-code-structure">1.1. Familiarize yourself with the code structure</h3>
<h3 id="12-outline-of-tasks">1.2. Outline of tasks</h3>
<h3 id="13-ray-sampling-5-points">1.3. Ray sampling (5 points)</h3>
<p><img src="data/xy_grid_0.png" alt="Grid"> <img src="data/rays_0.png" alt="Rays"></p>
<h3 id="14-point-sampling-5-points">1.4. Point sampling (5 points)</h3>
<p><img src="data/sample_points_0.png" alt="Points"></p>
<h3 id="15-volume-rendering-20-points">1.5. Volume rendering (20 points)</h3>
<p><img src="data/part_1.gif" alt="part_1"> <img src="data/depth_2.png" alt="depth"></p>
<h2 id="2-optimizing-a-basic-implicit-volume">2. Optimizing a basic implicit volume</h2>
<h3 id="21-random-ray-sampling-5-points">2.1. Random ray sampling (5 points)</h3>
<h3 id="22-loss-and-training-5-points">2.2. Loss and training (5 points)</h3>
<p>rounded to 2 deciamls</p>
<p>center of the box after training:(0.25, 0.25, 0.00)</p>
<p>side lengths of the box after training: (2.01, 1.50, 1.50)</p>
<h3 id="23-visualization">2.3. Visualization</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Trained</th>
<th style="text-align:center">TA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="data/part_2.gif" alt="part_2"></td>
<td style="text-align:center"><img src="data/part_2_ta.gif" alt="part_2_ta"></td>
</tr>
</tbody>
</table>
<h2 id="3-optimizing-a-neural-radiance-field-nerf-20-points">3. Optimizing a Neural Radiance Field (NeRF) (20 points)</h2>
<p><img src="data/part_3.gif" alt="part_3"></p>
<h2 id="4-nerf-extras-choose-one-more-than-one-is-extra-credit">4. NeRF Extras (CHOOSE ONE! More than one is extra credit)</h2>
<h3 id="41-view-dependence-10-points">4.1 View Dependence (10 points)</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Data</th>
<th style="text-align:center">Not View Dependent</th>
<th style="text-align:center">View Dependent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Lego</td>
<td style="text-align:center"><img src="data/lego_highres_part_3.gif" alt="lego_high_res"></td>
<td style="text-align:center"><img src="data/lego_highres_view_dependent_part_3.gif" alt="lego_high_res"></td>
</tr>
<tr>
<td style="text-align:center">Mateirals</td>
<td style="text-align:center"><img src="data/part_3_material.gif" alt="materials_high_res"></td>
<td style="text-align:center"><img src="data/materials_highres_view_dependent_part_3.gif" alt="materials_high_res_view_dependent"></td>
</tr>
</tbody>
</table>
<p>As we can see from the two high resolution dataset, with view dependet, the side view of lego feels more realistic and contrast. From materials dataset, we can see the refelction feels more metallic and realistic.</p>
<p>One thing to note down is that adding view dependet makes the model more complex and harder to train. Also may lead to potential overfit on some of the directions.</p>
<h3 id="42-coarsefine-sampling-10-points">4.2 Coarse/Fine Sampling (10 points)</h3>
<h1 id="b-neural-surface-rendering-50-points">B. Neural Surface Rendering (50 points)</h1>
<h2 id="5-sphere-tracing-10-points">5. Sphere Tracing (10 points)</h2>
<p><img src="data/part_5.gif" alt="part_5"></p>
<p>The algorithm starts initilizing points at camera origins, iteratively we march the points with signed distance along the ray. If a point hit the surface, we stop operating the point. We run the algorithm until all points hit the surface or max_iter reached. To prevent we left off some points, we still include points that are within the max range but not on the surface.</p>
<h2 id="6-optimizing-a-neural-sdf-15-points">6. Optimizing a Neural SDF (15 points)</h2>
<p><img src="data/part_6_input.gif" alt="part_6_input"> <img src="data/part_6.gif" alt="part_6"></p>
<p>My MLP has four chunks, the first chunk embed the points with harmonic embedding, after that a common MLP network has structure MLPWithInputSkips takes in the embedding, and send it to a distance network has structure MLPWithInputSkips, and another color network has 3 linear layers.</p>
<p>Eikonol loss takes in the neural network's gradient, calculates its norm and make it close to 1 as possible. This helps the neural network to generates smoother surface. Taking the mean helps the loss to be not too big, ensure smooth training also.</p>
<h2 id="7-volsdf-15-points">7. VolSDF (15 points)</h2>
<p>From <a href="https://arxiv.org/abs/2106.12052">VolSDF</a> paper, we have two formulas:</p>
<p><img src="data/f1.png" alt="f1"></p>
<p><img src="data/f2.png" alt="f2"></p>
<p>where sigma is the density, alpha and beta are the tuned parameters and omega is the signed distance function. Ψ_β is the Cumulative Distribution Function (CDF) of
the Laplace distribution with zero mean and β scale. Large beta would bias the surface to have smoother transition, and low beta would encourage sharper representation. With large beta the SDF will be easier to train because it has a more stable gradient. If I were to learn an accurate surface, I would choose low beta so I can catch more high frequency features.</p>
<p>The below is the result with default beta (0.05)</p>
<p><img src="data/part_7.gif" alt="part_7_rgb"> <img src="data/part_7_geometry.gif" alt="part_7_mesh"></p>
<p>The below is the result with 10 times beta (0.5)</p>
<p><img src="data/part_7_beta05.gif" alt="large_beta"> <img src="data/part_7_geometry_beta05.gif" alt="large_beta"></p>
<p>Changed hyperparameters: n_harmonic_functions_xyz: 6 -&gt; 4, n_layers_distance: 6 -&gt; 2</p>
<p>Less layers helped us to train faster, I should not lower n_harmonic_functions_xyz, this reduced the details. With larger n_harmonic_functions_xyz I should be able to produce better results.</p>
<h2 id="8-neural-surface-extras-choose-one-more-than-one-is-extra-credit">8. Neural Surface Extras (CHOOSE ONE! More than one is extra credit)</h2>
<h3 id="81-render-a-large-scene-with-sphere-tracing-10-points">8.1. Render a Large Scene with Sphere Tracing (10 points)</h3>
<h3 id="82-fewer-training-views-10-points">8.2 Fewer Training Views (10 points)</h3>
<table>
<thead>
<tr>
<th>Settings</th>
<th>NeRF</th>
<th>VolSDF</th>
<th>VolSDF Geometry</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full dataset</td>
<td><img src="data/part_3.gif" alt="part_3"></td>
<td><img src="data/part_7.gif" alt="part_7"></td>
<td><img src="data/part_7_geometry.gif" alt="part_7_geometry"></td>
</tr>
<tr>
<td>1/10 dataset</td>
<td><img src="data/part_3_10.gif" alt="part_3_10"></td>
<td><img src="data/part_7_10.gif" alt="part_7_10"></td>
<td><img src="data/part_7_geometry_10.gif" alt="part_7_geometry_10"></td>
</tr>
</tbody>
</table>
<p>As we can see, with fewer trainging views, both methonds degrades. From the red lights on top of the bulldozer and the details in the back of the bulldozer, we can tell that NeRF degrades more significantly. However, NeRF still captures more detail than VolSDF.</p>
<h3 id="83-alternate-sdf-to-density-conversions-10-points">8.3 Alternate SDF to Density Conversions (10 points)</h3>

            
            
        </body>
        </html>